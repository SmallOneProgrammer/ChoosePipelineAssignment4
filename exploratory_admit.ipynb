{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exploratory import Explore, ExploreTrain\n",
    "import pandas as pd\n",
    "\n",
    "explore_admit = Explore('data/admit.csv')\n",
    "\n",
    "df_admit, features_admit = explore_admit.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at the shape of the data\n",
    "df_admit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are a ton of rows and only 12 features\n",
    "#lets look at the features\n",
    "features_admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look to see if there are any NA values to drop\n",
    "\n",
    "df_admit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#While 2008 is a lot of values, it is small compared to 100 000 rows, lets go ahead and drop the na values\n",
    "\n",
    "df_admit = explore_admit.drop_na(df_admit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at the shape one more time\n",
    "\n",
    "df_admit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still a ton of rows\n",
    "#lets look at the distribution of the dataset\n",
    "\n",
    "ExploreTrain.check_distribution(df_admit, features_admit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets examine the features more closely\n",
    "explore_admit.display_chart(df_admit, features_admit, 'Admission Decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is clearly leakage from deposit paid as evidenced by 100% acceptance rate when deposit is paid\n",
    "#lets see if there is any other leakages by making DAGS\n",
    "\n",
    "#RL -> Admission Decision -> Deposit Paid\n",
    "#EA -> Admission Decision -> Deposit Paid\n",
    "#GPA -> Admission Decision -> Deposit Paid\n",
    "#SAT -> Admission Decision -> Deposit Paid\n",
    "#EA -> Application Type -> Application Fee -> Interview Feedback -> Admission Decision -> Deposit Paid\n",
    "#GPA -> Application Type -> Application Fee -> Interview Feedback -> Admission Decision -> Deposit Paid\n",
    "#SAT -> Application Type -> Application Fee -> Interview Feedback -> Admission Decision -> Deposit Paid\n",
    "#RL -> Interview Feedback -> Admission Decision -> Deposit Paid\n",
    "#EA -> Interview Feedback -> Admission Decision -> Deposit Paid\n",
    "#GPA -> Interview Feedback -> Admission Decision -> Deposit Paid\n",
    "#SAT -> Interview Feedback -> Admission Decision -> Deposit Paid\n",
    "\n",
    "#clearly there is leakage from Deposit Paid, Application Type, Interview Feedback, Application Fee\n",
    "#lets drop these features\n",
    "df_admit = explore_admit.drop_columns(\n",
    "    df_admit, \n",
    "    ['Deposit Paid', 'Application Type', 'Interview Feedback', 'Application Fee']\n",
    ")\n",
    "\n",
    "#lets drop the row identifiers as well\n",
    "df_admit = explore_admit.drop_columns(\n",
    "    df_admit,\n",
    "    ['Student ID', 'First Name', 'Last Name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets now explore with pycaret \n",
    "ExploreTrain.pycaret_explore(df_admit, 'Admission Decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the class report shows a difference in recall between target classes\n",
    "# lets look at the dataset more closely particularly the target variable\n",
    "\n",
    "df_admit['Admission Decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is clearly some imbalance in the dataset, because of this the best usage of classification \n",
    "#would be ada boost as it is great for imbalanced datasets and has the best F1, kappa, and MCC score\n",
    "#lets look more carefully using ada boost\n",
    "ExploreTrain.use_specific_model_pycaret(df_admit, 'Admission Decision', 'ada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the metrics based on the classification may not be as good for accepting students (0 target) but \n",
    "#because the precision and recall is so high for denied students 0.765 and 0.915 for precision and recall (1 target)\n",
    "#it is clear that the model is very good at predicting denied students \n",
    "#this is likely due to the imbalance in the dataset, so we cannot use it to predict accepted students\n",
    "#with a precision and recall of (0.542 and 0.262 respectively)\n",
    "\n",
    "#this concludes our analysis of the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
